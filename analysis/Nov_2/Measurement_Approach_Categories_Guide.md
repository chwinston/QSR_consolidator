# Measurement Approach Categories Guide

**Document Purpose**: Standardized, mutually exclusive, collectively exhaustive (MECE) categories for AI project KPI measurement approaches across all business units.

**Total Approaches Categorized**: 95
**Unique Categories**: 20
**Date**: November 3, 2025

---

## Category Definitions

### 1. Volume Completed (26 instances)
**Definition**: Count of tasks, tickets, story points, prompts, or other discrete items completed

**Examples**:
- Ticket count satisfactorily closed by Agent/Rep
- Increase in completed story points or tasks per developer
- Number of prompts sent into the AI
- Weekly tasks completed
- Number of upsold Insights Modules

**Common Keywords**: number of, count, tasks completed, story points, prompts, # of

---

### 2. Revenue Impact (10 instances)
**Definition**: Direct revenue measurement in dollars (ARR, sales, upsells)

**Examples**:
- ARR attributed to this feature
- Additional revenue from new customers
- Total ARR sold in USD
- Revenue from customers that upgrade

**Common Keywords**: revenue, ARR, sales, upsold, paying customers, $, USD

---

### 3. Time Saved (7 instances)
**Definition**: Hours or minutes saved per task or in total

**Examples**:
- Current hours devoted to creating content...cut that time in half
- Effective Hours Saved (EHS) per week
- Reduction in time required to create a customized demo
- Average time saved per task vs. baseline

**Common Keywords**: hours saved, time saved, reduction in time, minutes, EHS

---

### 4. Satisfaction Score (7 instances)
**Definition**: Satisfaction measured via surveys, ratings, or feedback

**Examples**:
- Staff Satisfaction surveys will be conducted
- Callers leave a short feedback note/rating
- Active Sign in for users
- CSAT surveys

**Common Keywords**: satisfaction survey, feedback, rating, CSAT, perception

---

### 5. Response Time (6 instances)
**Definition**: Time to respond, resolve, or handle cases/tickets

**Examples**:
- Reduction in Average Call Handling time
- Average resolution time for member support cases
- Average time to resolve (business hours)
- Reduction in average case resolution times

**Common Keywords**: response time, resolution time, handling time, turnaround time, time to resolve

---

### 6. License/Access Rate (5 instances)
**Definition**: Percentage of eligible FTE/employees with license or tool access

**Examples**:
- Tracking the number of licences issued as a % of near shore headcount
- Confirm all 6 have access
- % of employees actively using approved AI tools (with access)

**Common Keywords**: % of headcount, % of FTE, % near shore, licences issued, have access

---

### 7. Adoption Rate (5 instances)
**Definition**: Generic adoption rate measurement (percentage using or adopting)

**Examples**:
- Adoption rate %
- Measure the amount of leads received compared to valid leads
- First step is to measure use cases

**Common Keywords**: adoption rate, % adoption

---

### 8. Active Usage Rate (4 instances)
**Definition**: Percentage of users actively using based on frequency (weekly, daily)

**Examples**:
- % of R&D team actively using Claude Code weekly
- 75% adaptation across Microsoft Office suite
- 75% of Developers using Q Developer in weekly sprints

**Common Keywords**: actively using, weekly active, active use, weekly sprints, adaptation across

---

### 9. Customer Adoption Count (4 instances)
**Definition**: Number of customers or clients using a feature

**Examples**:
- Total number of customers using the agent
- The total number of members using the chat bot
- Total # of clients with the Advanced Email Campaigns module enabled

**Common Keywords**: number of customers, total # of clients, customers using, clients with

---

### 10. Customer Adoption Rate (4 instances)
**Definition**: Percentage of customers using or adopting a feature

**Examples**:
- % of clients with Advanced Email Campaigns enabled who are actively using
- Track % of users who adopt AI scheduling vs manual methods
- Feature activation rate (% of eligible users who begin using this feature)

**Common Keywords**: % of customers, % of clients, % of users who adopt, activation rate

---

### 11. Performance Benchmark (3 instances)
**Definition**: Comparison to industry standard, baseline, or competitor performance

**Examples**:
- System performance in ms 50% faster than ClubWise Dashboard
- Reduce to the Industry Average of 8-10 mins
- Code delivery velocity increases by 50% vs benchmark

**Common Keywords**: vs benchmark, vs baseline, faster than, industry average, industry standard

---

### 12. Escalation Rate (3 instances)
**Definition**: Percentage of interactions escalated or transferred to humans

**Examples**:
- percentage of chats that are transferred to a live agent
- Comparison of the number of calls routed to support
- 20% routed to support

**Common Keywords**: transferred to, routed to support, escalation, calls routed

---

### 13. Cycle Time Reduction (2 instances)
**Definition**: Reduction in process duration or cycle time (percentage or absolute)

**Examples**:
- Pre vs. post cycle time, % reduction achieved
- 50% reduction in cycle time for data migration process

**Common Keywords**: cycle time, reduction in cycle, process duration

---

### 14. Accuracy Rate (2 instances)
**Definition**: Percentage correct or error rate

**Examples**:
- 80% of correct answers
- Reduce errors in reconciliation cases by 20%

**Common Keywords**: % of correct, error, accuracy, reduce errors, correct answers

---

### 15. Self-Service Rate (2 instances)
**Definition**: Percentage of issues resolved without human intervention

**Examples**:
- Tickets that customers mark as self-solved
- Total number of inquiries handled by the chat bot without escalation to a support rep

**Common Keywords**: self-solved, without escalation, handled by automation

---

### 16. Conversion Rate (1 instance)
**Definition**: Percentage conversion between stages (leads to sales, demos to sales, etc.)

**Examples**:
- Conversion rate lift attributable to this feature
- Compare our conversion rate from before the use of the tool to after
- Conversion rate of demos completed to sales completed

**Common Keywords**: conversion rate, convert, demos to sales, uplift

---

### 17. Automation Rate (1 instance)
**Definition**: Percentage of workflow or process that is automated

**Examples**:
- % of workflow steps automated
- 75% of code delivered weekly created/reviewed using AI tools

**Common Keywords**: % automation, % of workflow, % automated, % reviewed

---

### 18. Cost Savings (1 instance)
**Definition**: Direct cost reduction in dollars

**Examples**:
- Since eliminating the Belay resource, we are saving $35K annually
- Cost of LMS less Cost of Scribe licences annually
- Estimated annual labor cost saved

**Common Keywords**: cost savings, saving $, annual savings, OPEX saving, labor cost saved

---

### 19. Proactive Touchpoints (1 instance)
**Definition**: Count of proactive outreach activities (emails, calls, meetings)

**Examples**:
- Weekly customer proactive touchpoints (emails, calls, and meetings)
- Weekly outreach to unique accounts

**Common Keywords**: proactive touchpoints, emails and calls, weekly outreach

---

### 20. Not Defined (1 instance)
**Definition**: Measurement approach not yet defined, under review, or TBD

**Examples**:
- Under review
- TBD
- Goal TBD

**Common Keywords**: under review, TBD, not have a baseline

---

## Category Distribution

| Rank | Category | Count | Percentage |
|------|----------|-------|------------|
| 1 | Volume Completed | 26 | 27.4% |
| 2 | Revenue Impact | 10 | 10.5% |
| 3 | Time Saved | 7 | 7.4% |
| 3 | Satisfaction Score | 7 | 7.4% |
| 5 | Response Time | 6 | 6.3% |
| 6 | License/Access Rate | 5 | 5.3% |
| 6 | Adoption Rate | 5 | 5.3% |
| 8 | Active Usage Rate | 4 | 4.2% |
| 8 | Customer Adoption Count | 4 | 4.2% |
| 8 | Customer Adoption Rate | 4 | 4.2% |
| 11 | Performance Benchmark | 3 | 3.2% |
| 11 | Escalation Rate | 3 | 3.2% |
| 13 | Cycle Time Reduction | 2 | 2.1% |
| 13 | Accuracy Rate | 2 | 2.1% |
| 13 | Self-Service Rate | 2 | 2.1% |
| 16 | Conversion Rate | 1 | 1.1% |
| 16 | Automation Rate | 1 | 1.1% |
| 16 | Cost Savings | 1 | 1.1% |
| 16 | Proactive Touchpoints | 1 | 1.1% |
| 16 | Not Defined | 1 | 1.1% |

**Total**: 95 measurement approaches across 20 categories

---

## Key Insights

### Most Common Approaches
1. **Volume Completed** (26) - Most businesses measure success by counting discrete outputs
2. **Revenue Impact** (10) - Direct financial impact is the second most common measure
3. **Time Saved** (7) and **Satisfaction Score** (7) - Tied for third, showing focus on efficiency and employee/customer happiness

### Underutilized Approaches
- **Cost Savings** (1) - Despite "Cost Savings" being a popular KPI category (18 instances), only 1 explicitly measures direct dollar savings
- **Automation Rate** (1) - Few businesses quantify the percentage of workflow automated
- **Conversion Rate** (1) - Limited use of funnel conversion metrics

### Category Pairs (Related but Distinct)
- **License/Access Rate** vs **Active Usage Rate**: Access (5) vs Active Use (4)
- **Customer Adoption Count** vs **Customer Adoption Rate**: Count (4) vs Percentage (4)
- **Time Saved** vs **Response Time**: Savings (7) vs Speed (6)
- **Time Saved** vs **Cycle Time Reduction**: Absolute (7) vs Relative (2)

### MECE Verification
✅ **Mutually Exclusive**: Each approach fits only one category
✅ **Collectively Exhaustive**: All 95 approaches categorized (including 1 "Not Defined")
✅ **Concise**: All category names are 2-4 words
✅ **Logical**: Categories group similar measurement methodologies

---

## Usage Notes

**For Analysis**: Use these categories to:
- Compare measurement sophistication across business units
- Identify measurement approach gaps
- Benchmark against best practices
- Develop measurement templates for specific KPI types

**For Standardization**: Encourage business units to:
- Use specific category names in future submissions
- Provide multiple measurement approaches from different categories
- Move away from generic "Volume Completed" toward more specific metrics

**File Location**: Applied in `analysis/Nov_2/KPI_measurement_quality.xlsx` column C (`measurement_approach_category`)
